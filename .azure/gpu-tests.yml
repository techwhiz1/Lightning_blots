trigger:
  tags:
    include:
      - '*'
  branches:
    include:
      - master
      - release/*
      - refs/tags/*
pr:
  - master
  - release/*

jobs:
  - job: testing
    strategy:
      matrix:
        'torch v1.13 | py3.8':
          PYTHON_VERSION: '3.8'
          TORCH_VERSION: '1.13.1'
          CUDA_VERSION_MM: '117'
          runner_image: "nvidia/cuda:11.7.1-cudnn8-runtime-ubuntu20.04"
        'torch v2.0 | py3.9':
          PYTHON_VERSION: '3.9'
          TORCH_VERSION: '2.0.1'
          CUDA_VERSION_MM: '117'
          runner_image: "nvidia/cuda:11.7.1-cudnn8-runtime-ubuntu20.04"
        'torch v2.0 | py3.10':
          PYTHON_VERSION: '3.10'
          TORCH_VERSION: '2.0.1'
          CUDA_VERSION_MM: '121'
          runner_image: "nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04"
    # how long to run the job before automatically cancelling
    timeoutInMinutes: "75"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"

    variables:
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0,1"; print(gpus)' )
      DEBIAN_FRONTEND: "noninteractive"
      TZ: "Europe/Amsterdam"
      PIP_CACHE_DIR: "/var/tmp/pip"
      TORCH_HOME: "/var/tmp/torch"
      TRANSFORMERS_CACHE: "/var/tmp/huggingface"
      DATASETS_PATH: "/var/tmp/bolts_datasets"  # custom for running Bolts
      FREEZE_REQUIREMENTS: 1

    container:
      #image: "pytorchlightning/pytorch_lightning:base-cuda-py3.9-torch1.12-cuda11.6.1"
      #image: "pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime"
      #image: "nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04"
      image: $(runner_image)
      #endpoint: azureContainerRegistryConnection
      options: "--gpus=all --shm-size=16g -v /usr/bin/docker:/tmp/docker:ro -v /var/tmp:/var/tmp"
    pool: "lit-rtx-3090"

    workspace:
      clean: all

    steps:

    - script: |
        container_id=$(head -1 /proc/self/cgroup|cut -d/ -f3)
        /tmp/docker exec -t -u 0 $container_id \
          sh -c "apt-get update && DEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::="--force-confold" -y install sudo"
        echo "##vso[task.setvariable variable=CONTAINER_ID]$container_id"
      displayName: 'Install Sudo in container (thanks Microsoft!)'

    - bash: |
        set -e
        #add-apt-repository ppa:deadsnakes/ppa
        apt-get -y update -qq --fix-missing
        apt-get -y install --no-install-recommends \
          build-essential \
          python$PYTHON_VERSION \
          python$PYTHON_VERSION-dev \
          python3-distutils \
          unrar \
          cmake \
          pciutils \
          swig \
          zlib1g-dev \
          wget curl
        update-alternatives --install /usr/bin/python python /usr/bin/python$PYTHON_VERSION 1

        wget https://bootstrap.pypa.io/get-pip.py
        python get-pip.py

        echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
      displayName: 'Install sys & python'  # CUDA image if completely blind

    - bash: |
        whereis nvidia
        nvidia-smi
        echo $CUDA_VISIBLE_DEVICES
        echo $CONTAINER_ID
        python --version
        pip --version
        pip list
      displayName: 'Image info & NVIDIA'

    - bash: |
        pip install "pip==21.2.3"  # todo: drop after resolving extras
        pip install ".[dev]" "torch==$TORCH_VERSION" \
          --prefer-binary -f https://download.pytorch.org/whl/cu${CUDA_VERSION_MM}/torch_stable.html
      displayName: 'Install pkg & dependencies'

    - bash: |
        pip --version
        pip list
        python -c "import torch ; mgpu = torch.cuda.device_count() ; assert mgpu >= 2, f'GPU: {mgpu}'"
      displayName: 'Sanity check'

    - script: |
        set -ex
        mkdir -p _datasets
        cd _datasets
        curl http://www.atarimania.com/roms/Roms.rar -o Roms.rar
        unrar x -y Roms.rar
        python -m atari_py.import_roms ROMS
      displayName: Download ROMs

    - bash: |
        # todo: optimize runtime, mainly because of retina example
        python -m pytest tests/ -v --cov=pl_bolts --durations=30 \
          --junitxml=$(Build.StagingDirectory)/test-results.xml
      displayName: 'Testing'

    - bash: |
        python -m coverage report
        python -m coverage xml

        curl -Os https://uploader.codecov.io/latest/linux/codecov
        chmod +x codecov
        ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) --flags=gpu,pytest --name="GPU-coverage" --env=linux,azure
        ls -l
      displayName: 'Statistics'

    - task: PublishTestResults@2
      displayName: 'Publish test results'
      inputs:
        testResultsFiles: '$(Build.StagingDirectory)/test-results.xml'
        testRunTitle: '$(Agent.OS) - $(Build.DefinitionName) - Python $(python.version)'
      condition: succeededOrFailed()

    #- task: PublishCodeCoverageResults@1
    #  displayName: 'Publish coverage report'
    #  inputs:
    #    codeCoverageTool: 'Cobertura'
    #    summaryFileLocation: '$(Build.SourcesDirectory)/coverage.xml'
    #    reportDirectory: '$(Build.SourcesDirectory)/htmlcov'
    #    testRunTitle: '$(Agent.OS) - $(Build.BuildNumber)[$(Agent.JobName)] - Python $(python.version)'
    #  condition: succeededOrFailed()

    - bash: rm -rf $(Build.SourcesDirectory)
      condition: succeededOrFailed()
      displayName: 'Cleanup'
